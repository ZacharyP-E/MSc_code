{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9abe1d",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140ac1c",
   "metadata": {},
   "source": [
    "This notebook showcases the data analysis pipeline. It begins by extracting semantic information from communications data to form a structured data set. Following this, various predictive models are applied to the data for analysis. The models are then assessed for their validity and reliability. Lastly, the results are visualised to facilitate interpretation and reporting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc5704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Importing the required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import time\n",
    "\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from scipy.stats import linregress\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41754f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a timer to identify how long the script takes.\n",
    "start_time_all = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for exporting pandas data tables to markdown.\n",
    "def pandas_to_markdown(dataframe, filename: str, caption: str, reference: str):\n",
    "    # Convert the input DataFrame to a Markdown-formatted table, without the index column\n",
    "    markdown_df = dataframe.to_markdown(index=False)\n",
    "    \n",
    "    # Add a metadata string to the Markdown-formatted table containing the caption and reference ID\n",
    "    metadata_str = \"\\n\" + \"\\n: \" + caption + \" {#\" + reference + \"}\"\n",
    "    markdown_with_metadata = markdown_df + metadata_str\n",
    "    \n",
    "    # Write the Markdown-formatted table to a file with the given filename in the \"../tables/\" directory\n",
    "    with open(\"../tables/\" + filename + \".md\", \"w\") as text_file:\n",
    "        text_file.write(markdown_with_metadata)\n",
    "    \n",
    "    # Print a confirmation message to the console that the operation is complete\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953482e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of libraries included\n",
    "libraries = [\n",
    "    'numpy', 'pandas', 'matplotlib', 'seaborn', 'nltk', 'sklearn', 'xgboost', 'statsmodels'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store library names and versions\n",
    "library_versions = {}\n",
    "\n",
    "# Get version for each library\n",
    "for library in libraries:\n",
    "    try:\n",
    "        library_module = __import__(library)\n",
    "        version = library_module.__version__\n",
    "        library_versions[library] = version\n",
    "    except ImportError:\n",
    "        library_versions[library] = 'Not installed'\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "libraries_versions_df = pd.DataFrame(list(library_versions.items()), columns=['Library', 'Version'])\n",
    "\n",
    "# Display the DataFrame\n",
    "libraries_versions_df\n",
    "\n",
    "#Export to markdown\n",
    "pandas_to_markdown(libraries_versions_df, \"python_libraries_versions\", \"List of Python libraries and their versions used in the analysis.\", \"tbl-python-libraries-versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7302ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of iterations for cross-validation\n",
    "num_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c346c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbaf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9a921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9746ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required data\n",
    "inc_info = pd.read_csv('../data/final/inc_info.csv')\n",
    "uprn_data = pd.read_csv('../data/final/uprn/uprn_master.csv')\n",
    "message_data = pd.read_csv('../data/final/messages.csv')\n",
    "callsign = pd.read_csv('../data/callsign.csv')\n",
    "weather = pd.read_csv(\"../data/final/weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82076945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge each of the datasets\n",
    "inc_df = inc_info.merge(uprn_data, on=\"uprn\", how=\"inner\")\n",
    "inc_df = inc_df.merge(weather, on=\"vis_inc_num\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f413264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the internal area metric to M2\n",
    "inc_df[\"internalArea_m\"] = inc_df[\"internalArea\"] * 0.09290303997 # Converting from feet to m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null values from internal area column.\n",
    "inc_df = inc_df.dropna(subset=[\"internalArea_m\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caluclating the proportion of fire damage.\n",
    "inc_df[\"fire_dam_prop\"] = inc_df[\"fire_damage_area_median_value\"] / inc_df[\"internalArea_m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit values in the columns \"fire_dam_prop\" and \"other_dam_prop\" to a maximum of 1\n",
    "inc_df[\"fire_dam_prop\"] = np.clip(inc_df[\"fire_dam_prop\"], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ab05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_nums = list(inc_df[\"vis_inc_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52da545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering messages to only include those from incidents with data\n",
    "message_data = message_data[message_data[\"incident_number\"].isin(inc_nums)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb6a69",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e76c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the message time\n",
    "message_data[\"message_time\"] = pd.to_datetime(message_data[\"message_time\"].str.split('.').str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b494dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in the callsign data to identify the type of resource being referneced.\n",
    "message_data = message_data.merge(callsign, left_on = 'source', right_on='callsign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dcb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initating a list of incidents with multiple arrival times. \n",
    "multi_arrivals = []\n",
    "all_arrivals = pd.DataFrame(columns=['incident_number', 'message_time', 'source', 'response_time', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to extract these arrivals. \n",
    "for i in inc_nums:\n",
    "    temp_inc_df = message_data[message_data[\"incident_number\"] == i]\n",
    "    \n",
    "    temp_start_time = temp_inc_df[\"message_time\"].min()\n",
    "    \n",
    "    temp_arrivals = temp_inc_df[(temp_inc_df[\"message\"].str.contains(\"to 02 - In Attendance\")) & (temp_inc_df[\"operator_id\"] == \"SYS\")]\n",
    "    \n",
    "    temp_arrivals = temp_arrivals.drop_duplicates(subset=[\"source\", \"message\"], keep=\"first\")\n",
    "    \n",
    "    if temp_arrivals[\"source\"].nunique() == temp_arrivals.shape[0]:\n",
    "        \n",
    "        temp_arrivals[\"response_time\"] = temp_arrivals[\"message_time\"] - temp_start_time\n",
    "        \n",
    "        temp_arrivals = temp_arrivals[[\"incident_number\", \"message_time\", \"source\", \"response_time\", 'Type']]\n",
    "        \n",
    "        all_arrivals = pd.concat([all_arrivals, temp_arrivals])\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        multi_arrivals.append(i)\n",
    "        \n",
    "\n",
    "    \n",
    "    del(temp_inc_df)\n",
    "    del(temp_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the response time to seconds.\n",
    "all_arrivals['response_seconds'] = all_arrivals['response_time'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b01732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caluclating how many of each resource were in atttendance.\n",
    "resources = all_arrivals.groupby(['Type', 'incident_number'])['source'].count().reset_index().pivot(columns = 'Type', index='incident_number').fillna(0).reset_index()\n",
    "\n",
    "resources.columns = resources.columns.droplevel()\n",
    "resources = resources.rename(columns={'': 'incident_number'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this information to the master dataset\n",
    "inc_df = inc_df.merge(resources, left_on=\"vis_inc_num\", right_on=\"incident_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892ef02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the response times for each incident number.\n",
    "grouped_times = all_arrivals.groupby('incident_number')['response_time'].apply(\n",
    "    lambda x: x.dt.total_seconds().dropna().astype(int).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56caf440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualting the entropy of response times by incident number.\n",
    "entropies = []\n",
    "for times in grouped_times:\n",
    "    counts = np.bincount(times)\n",
    "    probabilities = counts / np.sum(counts)\n",
    "    entropies.append(entropy(probabilities, base=2))\n",
    "\n",
    "# Add the entropies as a new column in the DataFrame\n",
    "all_arrivals['entropy'] = pd.Series(entropies)\n",
    "\n",
    "grouped_times = grouped_times.reset_index()\n",
    "grouped_times[\"entropy\"] = entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50440ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging this information back to the master dataset.\n",
    "inc_df = inc_df.merge(grouped_times, left_on=\"vis_inc_num\", right_on=\"incident_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b588b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336037a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27831663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7123a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c554b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92b2bc92",
   "metadata": {},
   "source": [
    "## Semantic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9472ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying messages relating to agencies\n",
    "agency_data = message_data[message_data[\"key_message_flag\"] == \"A\"]\n",
    "agency_data[\"count\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many agency messages by each incident number\n",
    "agency_count = agency_data.groupby(\"incident_number\")[\"count\"].sum().reset_index().rename(columns={\"count\":\"agency_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f516968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back to the master data.\n",
    "inc_df = inc_df.merge(agency_count, left_on=\"vis_inc_num\", right_on=\"incident_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dcba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9583dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting messages that were written by hand and formatting them\n",
    "hand_written = message_data[message_data[\"message\"].str.contains(\"Informative\")]\n",
    "\n",
    "hand_written[\"message\"] = hand_written[\"message\"].str.lower()\n",
    "\n",
    "hand_written[\"message\"] = hand_written[\"message\"].str.replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60086a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying a stopwords list\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Removing the stop words\n",
    "hand_written['message_without_stopwords'] = hand_written[\"message\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d6b478",
   "metadata": {},
   "source": [
    "### TF-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b51710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalising the TF-IDF model\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(hand_written[\"message_without_stopwords\"])\n",
    "z = pd.DataFrame(x.toarray(), columns = v.get_feature_names_out())\n",
    "z.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize kmeans with 3 centroids\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# fit the model\n",
    "kmeans.fit(x)\n",
    "# store cluster labels in a variable\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0529f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an elbow plot for the k means clusters\n",
    "random_state = 42\n",
    "\n",
    "# Calculate and store inertia (within-cluster sum of squares) for different values of k\n",
    "inertia_values = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "    kmeans.fit(x)\n",
    "    inertia = kmeans.inertia_\n",
    "    \n",
    "    inertia_values.append(inertia)\n",
    "\n",
    "# Plot the elbow curve\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "plt.plot(range(1, 11), inertia_values, marker='o')\n",
    "plt.title('Elbow Curve for KMeans on TF-IDF Data')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inertia', fontsize=12)\n",
    "plt.xticks(range(1, 11))\n",
    "\n",
    "plt.savefig(\"../figures/tf-idf-kmeans-elbow.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188d14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the TF-IDF data on the optimal number of clusters, and testing on other models. \n",
    "n_clusters = 3\n",
    "\n",
    "# Split data into training and test sets\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "kmeans_clusters = kmeans.fit_predict(x)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, kmeans_clusters_train, kmeans_clusters_test = train_test_split(x, kmeans_clusters, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=random_state)\n",
    "logreg.fit(x_train, kmeans_clusters_train)\n",
    "logreg_clusters = logreg.predict(x_test)\n",
    "\n",
    "# Create an XGBoost model\n",
    "xgb = XGBClassifier(random_state=random_state)\n",
    "xgb.fit(x_train, kmeans_clusters_train)\n",
    "xgb_clusters = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87683e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to hold cluster assignments\n",
    "classification_df = pd.DataFrame({\n",
    "    'K-Means': kmeans_clusters_test,\n",
    "    'Logistic Regression': logreg_clusters,\n",
    "    'XGBoost': xgb_clusters\n",
    "})\n",
    "\n",
    "# Calculate the number of times assignments were the same for each pair of methods\n",
    "identical_kmeans_logreg_count = (classification_df['K-Means'] == classification_df['Logistic Regression']).sum()\n",
    "identical_kmeans_xgb_count = (classification_df['K-Means'] == classification_df['XGBoost']).sum()\n",
    "identical_logreg_xgb_count = (classification_df['Logistic Regression'] == classification_df['XGBoost']).sum()\n",
    "\n",
    "# Calculate percentages of identical assignments for each pair\n",
    "identical_kmeans_logreg_percentage = (identical_kmeans_logreg_count / len(classification_df)) * 100\n",
    "identical_kmeans_xgb_percentage = (identical_kmeans_xgb_count / len(classification_df)) * 100\n",
    "identical_logreg_xgb_percentage = (identical_logreg_xgb_count / len(classification_df)) * 100\n",
    "\n",
    "identical_count = (classification_df['K-Means'] == classification_df['Logistic Regression']) & (classification_df['K-Means'] == classification_df['XGBoost'])\n",
    "identical_percentage = (identical_count.sum() / len(classification_df)) * 100\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "classification_result_df = pd.DataFrame({\n",
    "    'Pair Comparison': ['K-Means vs. Logistic Regression', 'K-Means vs. XGBoost', 'Logistic Regression vs. XGBoost', 'All Three Methods'],\n",
    "    'Percentage of Identical Cluster Assignments': [identical_kmeans_logreg_percentage, identical_kmeans_xgb_percentage, identical_logreg_xgb_percentage, identical_percentage]\n",
    "})\n",
    "classification_result_df = classification_result_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966985cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "hand_written_train, hand_written_test = train_test_split(hand_written, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Split 'x' data into training and test sets\n",
    "x_train, x_test = train_test_split(x, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Perform K-means clustering on the training data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "kmeans_clusters_train = kmeans.fit_predict(x_train)\n",
    "\n",
    "# Predict clusters on the test data\n",
    "kmeans_clusters_test = kmeans.predict(x_test)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=random_state)\n",
    "logreg.fit(x_train, kmeans_clusters_train)\n",
    "\n",
    "# Predict clusters on the test data using the trained Logistic Regression model\n",
    "logreg_clusters_test = logreg.predict(x_test)\n",
    "\n",
    "# Create an XGBoost model\n",
    "xgb = XGBClassifier(random_state=random_state)\n",
    "xgb.fit(x_train, kmeans_clusters_train)\n",
    "\n",
    "# Predict clusters on the test data using the trained XGBoost model\n",
    "xgb_clusters_test = xgb.predict(x_test)\n",
    "\n",
    "# Add the cluster assignments to the test set of 'hand_written' DataFrame\n",
    "hand_written_test['K-Means'] = kmeans_clusters_test\n",
    "hand_written_test['Logistic Regression'] = logreg_clusters_test\n",
    "hand_written_test['XGBoost'] = xgb_clusters_test\n",
    "\n",
    "# map clusters to appropriate labels \n",
    "cluster_map = {0: \"casualty info\", 1: \"incident info\", 2: \"after-action\"}\n",
    "\n",
    "# Apply cluster map to cluster columns\n",
    "for cluster_col in ['K-Means', 'Logistic Regression', 'XGBoost']:\n",
    "    hand_written_test[cluster_col] = hand_written_test[cluster_col].map(cluster_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32cb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a proportion of the cluster assignments for hand validation\n",
    "proportion_for_validation = 0.1\n",
    "hand_validation_df = hand_written_test[[\"message\"]].sample(frac=proportion_for_validation, random_state=random_state)\n",
    "hand_df = hand_written_test.sample(frac=proportion_for_validation, random_state=random_state)\n",
    "\n",
    "# Add a new 'hand' column to indicate hand-validation\n",
    "hand_validation_df['hand'] = \"\"\n",
    "\n",
    "# Save the hand validation data to an Excel file\n",
    "hand_validation_df.to_excel('../data/validate/hand_validation_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecaaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hand-validated data from the Excel file\n",
    "hand_validated_data = pd.read_excel('../data/validate/hand_validation_data_validated.xlsx')\n",
    "\n",
    "# Calculate error percentage for each clustering method\n",
    "def calculate_error_percentage(true_clusters, validated_clusters):\n",
    "    error_count = (true_clusters != validated_clusters).sum()\n",
    "    error_percentage = (error_count / len(true_clusters)) * 100\n",
    "    return error_percentage\n",
    "\n",
    "# Reset index of hand_df\n",
    "hand_df_reset = hand_df.reset_index(drop=True)\n",
    "\n",
    "# Calculate error percentage for each clustering method\n",
    "kmeans_error = calculate_error_percentage(hand_df_reset['K-Means'], hand_validated_data['hand'])\n",
    "logreg_error = calculate_error_percentage(hand_df_reset['Logistic Regression'], hand_validated_data['hand'])\n",
    "xgb_error = calculate_error_percentage(hand_df_reset['XGBoost'], hand_validated_data['hand'])\n",
    "\n",
    "# Print error percentages\n",
    "print(f\"K-Means Error Percentage: {kmeans_error:.2f}%\")\n",
    "print(f\"Logistic Regression Error Percentage: {logreg_error:.2f}%\")\n",
    "print(f\"XGBoost Error Percentage: {xgb_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb718e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error percentages\n",
    "error_data = {\n",
    "    \"Method\": [\"K-Means\", \"Logistic Regression\", \"XGBoost\"],\n",
    "    \"Error Percentage\": [kmeans_error, logreg_error, xgb_error]\n",
    "}\n",
    "\n",
    "error_df = pd.DataFrame(error_data)\n",
    "error_df = error_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the table results to markdown for reporting.\n",
    "pandas_to_markdown(error_df, \"tf-idf_clustering_error\", \"Comparison of error percentages for differenet clustering and classifiaciton methods using Manual Validation. Conducted on the manually transcribed incident log messages.\", \"tbl-tf-idf-clustering-error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37ca98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d336fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clusters\n",
    "hand_written['cluster'] = clusters\n",
    "\n",
    "\n",
    "# apply mapping\n",
    "hand_written['cluster'] = hand_written['cluster'].map(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f2f34d",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# pass our X to the pca and store the reduced vectors into pca_vecs\n",
    "pca_vecs = pca.fit_transform(x.toarray())\n",
    "# save our two dimensions into x0 and x1\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_written['cluster'] = clusters\n",
    "hand_written['x0'] = x0\n",
    "hand_written['x1'] = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(x.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = v.get_feature_names_out() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb02deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map clusters to appropriate labels \n",
    "cluster_map = {0: \"casualty info\", 1: \"incident info\", 2: \"after-action\"}\n",
    "# apply mapping\n",
    "hand_written['cluster'] = hand_written['cluster'].map(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image size\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "# set a title\n",
    "#plt.title(\"TF-IDF + KMeans\", fontdict={\"fontsize\": 18})\n",
    "# set axes names\n",
    "plt.xlabel(\"PC1\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"PC2\", fontdict={\"fontsize\": 16})\n",
    "# create scatter plot with seaborn, where hue is the class used to group the data\n",
    "sns.scatterplot(data=hand_written, x='x0', y='x1', hue='cluster', palette=\"tab10\", edgecolor=\"white\", alpha=0.5)\n",
    "\n",
    "plt.legend(loc=\"lower right\", title =\"Cluster\")\n",
    "\n",
    "\n",
    "ax.tick_params(left=False, bottom=False)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "plt.title(\"K-Means Cluster Analysis of TF-IDF: PCA Plot\")\n",
    "\n",
    "plt.savefig(\"../figures/tf-idf-pca.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7675a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c779a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09569450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e6ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6926717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of each message type\n",
    "y1 = hand_written.groupby([\"incident_number\", \"cluster\"])[\"message_time\"].nunique().reset_index().rename(columns={\"message_time\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4031f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting it to a pivot table\n",
    "y1_pivot = y1.pivot_table(values = \"count\", index = y1[\"incident_number\"], columns = \"cluster\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6307c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back to the master data\n",
    "inc_df = inc_df.merge(y1_pivot, left_on=\"vis_inc_num\", right_on=\"incident_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69539a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a42a823e",
   "metadata": {},
   "source": [
    "# Distribution of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e522d5",
   "metadata": {},
   "source": [
    "Plots describing the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ee54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = inc_df, x = \"fire_dam_prop\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Proportion of Building Damaged to Fire\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_fire_dam = inc_df[\"fire_dam_prop\"].median()\n",
    "mean_fire_dam = inc_df[\"fire_dam_prop\"].mean()\n",
    "\n",
    "plt.axvline(x=median_fire_dam, linewidth=2, color='b', label=f\"Median: {median_fire_dam:.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_fire_dam, linewidth=2, color='b', label=f\"Mean: {mean_fire_dam:.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "\n",
    "plt.xlim(0,1)\n",
    "\n",
    "plt.title(\"Distributuon of Proportion of Building Damaged to Fire\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/fire_dam.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79409",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = inc_df, x = \"fire_damage_area_median_value\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Area of Building Damaged to Fire ($m^2$)\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_fire_dam_area = inc_df[\"fire_damage_area_median_value\"].median()\n",
    "mean_fire_dam_area = inc_df[\"fire_damage_area_median_value\"].mean()\n",
    "\n",
    "plt.axvline(x=median_fire_dam_area, linewidth=2, color='b', label=f\"Median: {median_fire_dam_area:.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_fire_dam_area, linewidth=2, color='b', label=f\"Mean: {mean_fire_dam_area:.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(6.25))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.0025))\n",
    "\n",
    "plt.xlim(0, 225)\n",
    "\n",
    "plt.title(\"Distributuon of Area of Building Damaged to Fire\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/fire_dam_area.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e38419",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = inc_df, x = \"entropy\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Response Time Entropy\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_entropy = inc_df[\"entropy\"].median()\n",
    "mean_entropy = inc_df[\"entropy\"].mean()\n",
    "\n",
    "plt.axvline(x=median_entropy, linewidth=2, color='b', label=f\"Median: {median_entropy:.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_entropy, linewidth=2, color='b', label=f\"Mean: {mean_entropy:.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "\n",
    "plt.xlim(0,5)\n",
    "\n",
    "plt.title(\"Distribution of Response Time Entropy\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/entropy.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865786d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = inc_df, x = \"agency_count\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Count of Agency Messages\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_agency_count = inc_df[\"agency_count\"].median()\n",
    "mean_agency_count = inc_df[\"agency_count\"].mean()\n",
    "\n",
    "plt.axvline(x=median_agency_count, linewidth=2, color='b', label=f\"Median: {median_agency_count:.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_agency_count, linewidth=2, color='b', label=f\"Mean: {mean_agency_count:.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "plt.xlim(0,30)\n",
    "\n",
    "plt.title(\"Distribution of Count of Agnecy Messages\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/agency_count.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = inc_df, x = \"precipitation_sum\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Total Precipitation 7 Days Prior to Incidents (mm)\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_precip = inc_df[\"precipitation_sum\"].median()\n",
    "mean_precip = inc_df[\"precipitation_sum\"].mean()\n",
    "\n",
    "plt.axvline(x=median_precip, linewidth=2, color='b', label=f\"Median: {median_precip:.2f} mm\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_precip, linewidth=2, color='b', label=f\"Mean: {mean_precip:.2f} mm\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.0025))\n",
    "\n",
    "plt.xlim(0,120)\n",
    "\n",
    "plt.title(\"Distribution of Total Precipitation\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/precipitation.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = inc_df, x = \"mean_temperature\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Mean Temperature 7 Days Prior to Incidents (°C)\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_temp = inc_df[\"mean_temperature\"].median()\n",
    "mean_temp = inc_df[\"mean_temperature\"].mean()\n",
    "\n",
    "plt.axvline(x=median_temp, linewidth=2, color='b', label=f\"Median: {median_temp:.2f} °C\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_temp, linewidth=2, color='b', label=f\"Mean: {mean_temp:.2f} °C\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.025))\n",
    "\n",
    "plt.title(\"Distribution of Mean Temperature\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/temperature.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ba5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a14304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a71db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345f0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802fda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593614d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dac6eef",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129123c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create lists to store results\n",
    "coefficients = []\n",
    "r2_scores = []\n",
    "r2_score_x = []\n",
    "model_r2_scores = []\n",
    "p_values = []\n",
    "percentage_within_bounds_list = []\n",
    "real_predicted_values = []\n",
    "residuals_values = []\n",
    "rmse_values = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your dataset is in 'inc_df', create the feature matrix (X) and target variable (y)\n",
    "X = inc_df[['entropy', 'agency_count', 'after-action', 'casualty info', 'incident info', 'mean_temperature', 'precipitation_sum', 'windspeed', 'energyScore']]\n",
    "\n",
    "# Handle missing values\n",
    "X.dropna(inplace=True)\n",
    "y = inc_df['fire_dam_prop'][X.index]\n",
    "\n",
    "# Perform one-hot encoding for the categorical column (replace 'energyScore' with the actual column name)\n",
    "X = pd.get_dummies(X, columns=['energyScore'], drop_first=True, dtype=int)\n",
    "\n",
    "# Add a constant term to the independent variables to account for the intercept in the regression\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=np.random.randint(1000))\n",
    "    \n",
    "    # Fit the multiple linear regression model\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    \n",
    "    # Get the summary of the regression model\n",
    "    summary = model.summary()\n",
    "    \n",
    "    # Extract coefficients and p-values\n",
    "    model_params = model.params\n",
    "    pvals = model.pvalues\n",
    "    \n",
    "    # Get R-squared\n",
    "    model_r2 = model.rsquared\n",
    "    \n",
    "    # Generate predictions for the test set\n",
    "    test_mean_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R-squared based on predictive ability\n",
    "    r2_x = 1 - (np.sum((y_test - test_mean_predictions) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "    r2 = r2_score(y_test, test_mean_predictions)\n",
    "    \n",
    "    # Calculate the residuals for the test set\n",
    "    residuals = y_test - test_mean_predictions\n",
    "    \n",
    "    # Calculate the standard deviation of residuals (a measure of uncertainty)\n",
    "    residual_std = residuals.std()\n",
    "    \n",
    "    # Calculate RMSE for the test set\n",
    "    rmse = sqrt(mean_squared_error(y_test, test_mean_predictions))\n",
    "\n",
    "    # Append the RMSE value to the list\n",
    "    rmse_values.append(rmse)\n",
    "    \n",
    "    # Calculate the range of predictions (within one standard deviation) for the test set\n",
    "    test_lower_bound = test_mean_predictions - residual_std\n",
    "    test_upper_bound = test_mean_predictions + residual_std\n",
    "    \n",
    "    # Count the number of predictions within the bounds\n",
    "    num_within_bounds = ((y_test >= test_lower_bound) & (y_test <= test_upper_bound)).sum()\n",
    "    \n",
    "    # Calculate the percentage of predictions within the bounds\n",
    "    percentage_within_bounds = num_within_bounds / len(y_test) * 100\n",
    "    \n",
    "    # Append results to lists\n",
    "    coefficients.append(model_params)\n",
    "    r2_scores.append(r2)\n",
    "    r2_score_x.append(r2_x)\n",
    "    model_r2_scores.append(model_r2)\n",
    "    p_values.append(pvals)\n",
    "    percentage_within_bounds_list.append(percentage_within_bounds)\n",
    "    \n",
    "    # Append real and predicted values to the list\n",
    "    real_predicted_values.append(pd.DataFrame({'Real': y_test, 'Predicted': test_mean_predictions}))\n",
    "    \n",
    "    # Append the residuals to the list\n",
    "    residuals_values.append(pd.DataFrame({'Residuals': residuals}))\n",
    "\n",
    "# Create a DataFrame to store real and predicted values from each iteration\n",
    "real_predicted_df = pd.concat(real_predicted_values, axis=0)\n",
    "\n",
    "residuals_values_df = pd.concat(residuals_values, axis=0)\n",
    "\n",
    "\n",
    "# Create DataFrames to store the results\n",
    "coefficients_df = pd.DataFrame(coefficients)\n",
    "\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Predictive R-squared': r2_scores,\n",
    "    'Other Predictive R-squared': r2_score_x,\n",
    "    'Model R-squared': model_r2_scores,\n",
    "    'RMSE': rmse_values,  # Here you include the RMSE values\n",
    "    'Percentage within Bounds': percentage_within_bounds_list\n",
    "})\n",
    "\n",
    "# Print or analyze the DataFrames as needed\n",
    "#coefficients_df\n",
    "#p_values_df\n",
    "#results_summary\n",
    "\n",
    "# Calculate the duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Print the duration\n",
    "print(\"Total duration: {:.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d707c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to compare real and predicted values\n",
    "real_predicted_df = real_predicted_df[(real_predicted_df[\"Predicted\"] >= 0) & (real_predicted_df[\"Predicted\"] <= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the proportion of data that is overestimated.\n",
    "print(str(round(real_predicted_df[real_predicted_df['Predicted'] > real_predicted_df['Real']].shape[0] \n",
    "                / real_predicted_df.shape[0] * 100, 2)) +\"% overestimated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the difference between the predicted and actual data.\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "# Calculate the slope and intercept for the line of best fit\n",
    "slope, intercept = np.polyfit(real_predicted_df['Real'], real_predicted_df['Predicted'], 1)\n",
    "\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "\n",
    "sns.scatterplot(data = real_predicted_df, x=\"Real\", y=\"Predicted\", alpha=0.05, edgecolor=\"lightblue\", color=\"None\", ax=ax)\n",
    "plt.plot([0, 1], [intercept, slope + intercept], color='blue', linestyle='-', label= f'Predicted = {slope:.2f} * Real + {intercept:.2f}')\n",
    "\n",
    "plt.xlabel(\"Real\", fontsize=12)\n",
    "plt.ylabel(\"Predicted\", fontsize=12)\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "\n",
    "plt.title('Scatter Plot with Lines of Best Fit for Multilinear regression', y=1.05)\n",
    "plt.suptitle(\"Predicting Proportion of Property Damage\", y=0.92, size=10)\n",
    "\n",
    "plt.savefig(\"../figures/scatter/linear_real_predicted_prop_damage.png\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Anderson-Darling test for normal distribution of residuals\n",
    "p_value = normal_ad(residuals_values_df[\"Residuals\"])[1]\n",
    "print(\"p-value from the test - below 0.05 generally means non-normal:\", p_value)\n",
    "\n",
    "# Reporting the normality of the residuals\n",
    "if p_value < 0.05:\n",
    "    print(\"Residuals are not normally distributed\")\n",
    "else:\n",
    "    print(\"Residuals are normally distributed\")\n",
    "\n",
    "# Plotting the residuals distribution\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "sns.kdeplot(residuals_values_df[\"Residuals\"], color=\"lightblue\", fill=True, ax=ax)\n",
    "\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "plt.axvline(x=residuals_values_df[\"Residuals\"].median(), linewidth=2, color='b', label=f\"Median: {residuals_values_df['Residuals'].median():.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=residuals_values_df[\"Residuals\"].mean(), linewidth=2, color='b', label=f\"Mean: {residuals_values_df['Residuals'].mean():.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.0625))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.125))\n",
    "\n",
    "plt.title(\"Distribution of risiduals for linear model predicting proportion of building burned\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "\n",
    "plt.savefig(\"../figures/distributions/linear_risiduals_prop_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "p_value_thresh=0.05\n",
    "\n",
    "print()\n",
    "if p_value > p_value_thresh:\n",
    "    print('Assumption satisfied')\n",
    "else:\n",
    "    print('Assumption not satisfied')\n",
    "    print()\n",
    "    print('Confidence intervals will likely be affected')\n",
    "    print('Try performing nonlinear transformations on variables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "sns.heatmap(X.iloc[:,1:].corr(), ax=ax, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Correlation of Variables\")\n",
    "\n",
    "plt.savefig(\"../figures/linear_correlation_prop_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ed508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the VIF for each variable\n",
    "VIF = [variance_inflation_factor(X.iloc[:,1:], i) for i in range(X.iloc[:,1:].shape[1])]\n",
    "\n",
    "# Create a DataFrame to store VIF values with variable names\n",
    "vif_df = pd.DataFrame({\n",
    "    'Variable': X.iloc[:,1:].columns,\n",
    "    'VIF': VIF\n",
    "})\n",
    "\n",
    "# Round the VIF values to 2 decimal places\n",
    "vif_df['VIF'] = vif_df['VIF'].round(2)\n",
    "\n",
    "vif_df\n",
    "\n",
    "pandas_to_markdown(vif_df, \"linear_vif_prop_damage\", \"Variance Inflation Factors (VIF) table showing multicollinearity among predictor variables. Higher VIF values indicate stronger multicollinearity, potentially affecting the interpretability and reliability of regression coefficients. Note: The values presented are rounded to two decimal places.\", \"tbl-linear_vif_prop_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d068443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "durbinWatson = durbin_watson(residuals_values_df[\"Residuals\"])\n",
    "\n",
    "# Interpretation based on the Durbin-Watson score\n",
    "if durbinWatson < 1.5:\n",
    "    interpretation = 'Signs of positive autocorrelation. Assumption not satisfied. Consider adding lag variables.'\n",
    "elif durbinWatson > 2.5:\n",
    "    interpretation = 'Signs of negative autocorrelation. Assumption not satisfied. Consider adding lag variables.'\n",
    "else:\n",
    "    interpretation = 'Little to no autocorrelation. Assumption satisfied.'\n",
    "\n",
    "# Create a DataFrame\n",
    "autocorrelation_table = pd.DataFrame({\n",
    "    'Durbin Watson Score': [durbinWatson],\n",
    "    'Interpretation': [interpretation]\n",
    "})\n",
    "\n",
    "# Round the VIF values to 2 decimal places\n",
    "autocorrelation_table['Durbin Watson Score'] = autocorrelation_table['Durbin Watson Score'].round(2)\n",
    "\n",
    "# Print the DataFrame\n",
    "autocorrelation_table\n",
    "\n",
    "pandas_to_markdown(autocorrelation_table, \"linear_durbinWatson_prop_damage\", \"The table displays the calculated Durbin-Watson score and its corresponding interpretation based on autocorrelation assumptions. The Durbin-Watson score measures the presence of autocorrelation in the model residuals. Note: The values presented are rounded to two decimal places.\", \"tbl-linear-durbinWatson-prop-damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_values_df[residuals_values_df[\"Residuals\"]>=0].shape[0] / residuals_values_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_values_df[residuals_values_df[\"Residuals\"]<=0].shape[0] / residuals_values_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac5927",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.scatterplot(data = residuals_values_df, x = residuals_values_df.index, y=\"Residuals\", alpha=0.05, edgecolor=\"lightblue\", color=\"None\", ax = ax)\n",
    "plt.axhline(y = 0, color='darkorange', linestyle='--')\n",
    "\n",
    "plt.ylim(-0.9, 0.9)\n",
    "\n",
    "plt.xlabel(\"Data Point Index\", fontsize=12)\n",
    "plt.ylabel(\"Residuals\", fontsize=12)\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(12.5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "\n",
    "plt.title(\"Distribution of Residuals for Linear Prediction of Proportion of Building Damage\")\n",
    "\n",
    "plt.savefig(\"../figures/scatter/linear_risiduals_prop_damage.png\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191adfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06733dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ec7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a61c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacb7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51912fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e6a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc16dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = results_summary[results_summary[\"Predictive R-squared\"] > -10], x = \"Predictive R-squared\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Predictive $R^2$ Value\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_value = results_summary[results_summary[\"Predictive R-squared\"] > -10][\"Predictive R-squared\"].median()\n",
    "mean_value = results_summary[results_summary[\"Predictive R-squared\"] > -10][\"Predictive R-squared\"].mean()\n",
    "\n",
    "plt.axvline(x=median_value, linewidth=2, color='b', label=f\"Median: {median_value:.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_value, linewidth=2, color='b', label=f\"Mean: {mean_value:.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "\n",
    "plt.title(\"Distribution of predictive $R^2$ values for linear model predicting proportion of building burned\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/linear_r2_prop_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f6410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_coefficients_df = coefficients_df.describe().T.round(3)\n",
    "rounded_coefficients_df.columns = rounded_coefficients_df.columns.str.capitalize()\n",
    "rounded_coefficients_df.drop(columns=['Count'], inplace=True)\n",
    "rounded_coefficients_df = rounded_coefficients_df.reset_index()\n",
    "rounded_coefficients_df = rounded_coefficients_df[[\"index\", \"Mean\", \"50%\",\"Std\"]]\n",
    "rounded_coefficients_df = rounded_coefficients_df.rename(columns={\"index\": \"Independent Variable\", \n",
    "                                                                  \"Mean\":\"Mean Coefficient Value\",\n",
    "                                                                 \"50%\": \"Median Coefficient Value\",\n",
    "                                                                 \"Std\": \"Standard Deviation\"})\n",
    "\n",
    "\n",
    "pandas_to_markdown(rounded_coefficients_df, \"linear_coefficients_prop_damage\", \"Table presenting summary statistics of coefficients obtained from a multiple linear regression analysis. The statistics presented offer insight into the distribution and central tendency of the coefficients, allowing the relationship of independent and target variables to be discovered. Note: The values presented are rounded to three decimal places.\", \"tbl-linear_coefficients_prop_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21f779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d817e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_df_melt = pd.melt(coefficients_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b319a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_df_melt = coefficient_df_melt[coefficient_df_melt[\"variable\"] != \"const\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_df_melt[\"variable\"] = coefficient_df_melt[\"variable\"].replace({\"entropy\": \"arrival_time_entropy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data= coefficient_df_melt, ax=ax, color=\"lightblue\")\n",
    "\n",
    "plt.xlabel(\"Feature\", fontsize=12)\n",
    "plt.ylabel(\"Coefficient Value\", fontsize=12)\n",
    "\n",
    "\n",
    "#median_value = results_summary[results_summary[\"Predictive R-squared\"] > -10][\"Predictive R-squared\"].median()\n",
    "#mean_value = results_summary[results_summary[\"Predictive R-squared\"] > -10][\"Predictive R-squared\"].mean()\n",
    "\n",
    "#plt.axvline(x=median_value, linewidth=2, color='b', label=f\"Median: {median_value:.2f}\", alpha=0.5, linestyle=\":\")\n",
    "#plt.axvline(x=mean_value, linewidth=2, color='b', label=f\"Mean: {mean_value:.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "plt.xticks(rotation=45, ha = \"right\", rotation_mode='anchor')\n",
    "\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "\n",
    "plt.title(\"Distribution of coefficient values for linear model predicting proportion of building burned\")\n",
    "\n",
    "#plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/linear_constants_prop_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62241c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_df_melt[coefficient_df_melt[\"variable\"].str.contains(\"energy\")][\"value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e76b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b761f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b164b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_summary[\"Predictive R-squared\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc154cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'data' and a column named 'values'\n",
    "threshold = 0\n",
    "\n",
    "# Count the number of data points above the threshold\n",
    "count_above_threshold = results_summary[results_summary[\"Predictive R-squared\"] > threshold].shape[0]\n",
    "\n",
    "# Calculate the total number of data points\n",
    "total_data_points = results_summary.shape[0]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_above_threshold = (count_above_threshold / total_data_points) * 100\n",
    "\n",
    "print(percentage_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2 = results_summary[\"Predictive R-squared\"].mean()\n",
    "std_r2 = results_summary[\"Predictive R-squared\"].std()\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean R^2^\", \"Standard Deviation of R^2^\"],\n",
    "    \"Value\": [mean_r2, std_r2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858db48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb5ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a976259e",
   "metadata": {},
   "source": [
    "## Non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9dacb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Initialize a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Create lists to store results\n",
    "model_names = []\n",
    "mse_values = []\n",
    "rmse_values = []\n",
    "r2_values = []\n",
    "\n",
    "\n",
    "\n",
    "# Specify the models and their corresponding constructors\n",
    "models = [\n",
    "    #('Neural Network', MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)),\n",
    "    ('Random Forest', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "    ('SVR', SVR(kernel='rbf', C=1.0, epsilon=0.1)),\n",
    "    ('KNN Regression', KNeighborsRegressor(n_neighbors=5)),\n",
    "    ('XGBoost', XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3))\n",
    "]\n",
    "\n",
    "# Iterate through the models\n",
    "for model_name, model in models:\n",
    "    # Create a list to store model results for each iteration\n",
    "    model_results = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Split the data into training and testing sets (80% train, 20% test)\n",
    "        X_train_nonlinear, X_test_nonlinear, y_train_nonlinear, y_test_nonlinear = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        # Fit the model and make predictions\n",
    "        model.fit(X_train_nonlinear, y_train_nonlinear)\n",
    "        y_pred_nonlinear = model.predict(X_test_nonlinear)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse_nonlinear = mean_squared_error(y_test_nonlinear, y_pred_nonlinear)\n",
    "        rmse_nonlinear = mean_squared_error(y_test_nonlinear, y_pred_nonlinear, squared=False)\n",
    "        r2_nonlinear = r2_score(y_test_nonlinear, y_pred_nonlinear)\n",
    "\n",
    "        # Append results to the model_results list\n",
    "        model_results.append((mse_nonlinear, rmse_nonlinear, r2_nonlinear))\n",
    "\n",
    "    # Append results to the respective lists\n",
    "    model_names.extend([model_name] * num_iterations)\n",
    "    mse_values.extend([result[0] for result in model_results])\n",
    "    rmse_values.extend([result[1] for result in model_results])\n",
    "    r2_values.extend([result[2] for result in model_results])\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "nonlinear_model_results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2^': r2_values,\n",
    "    'MSE': mse_values,\n",
    "    'RMSE': rmse_values\n",
    "})\n",
    "\n",
    "# Round the values\n",
    "nonlinear_model_results_df = nonlinear_model_results_df.round(3)\n",
    "\n",
    "# Display the DataFrame\n",
    "nonlinear_model_results_df\n",
    "\n",
    "# Calculate the duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Print the duration\n",
    "print(\"Total duration: {:.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model_results_df_1 = nonlinear_model_results_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42669b55",
   "metadata": {},
   "source": [
    "## Comparing Linear and Non-Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_results_df = results_summary[[\"Predictive R-squared\"]]\n",
    "linear_model_results_df[\"Model\"] = \"Multi-linear \\nRegression\"\n",
    "linear_model_results_df = linear_model_results_df.rename(columns={\"Predictive R-squared\": \"R^2^\"})\n",
    "linear_model_results_df = linear_model_results_df[[\"Model\", \"R^2^\"]]\n",
    "\n",
    "nonlinear_model_results_df = nonlinear_model_results_df[[\"Model\", \"R^2^\"]]\n",
    "\n",
    "all_model_results_df = pd.concat([linear_model_results_df, nonlinear_model_results_df])\n",
    "\n",
    "\n",
    "# Calculate median R^2^ values for each model\n",
    "median_r2_group = all_model_results_df.groupby(\"Model\")[\"R^2^\"].median().sort_values()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "#plt.axhline(y=0, linewidth=3, color='black', alpha=0.1)\n",
    "\n",
    "sns.boxplot(data = all_model_results_df, x=\"Model\", y=\"R^2^\", ax=ax, color=\"lightblue\", order = median_r2_group.reset_index()[\"Model\"].to_list())\n",
    "\n",
    "plt.ylabel(\"Predictive $R^2$ Value\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "#plt.legend(title=\"Key\", loc=\"lower left\")\n",
    "\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.125))\n",
    "\n",
    "plt.title(\"$R^2$ Value of Models Predicting Proportion of Fire Damage\")\n",
    "\n",
    "plt.ylim(-2, 0.6)\n",
    "\n",
    "plt.savefig(\"../figures/distributions/all_model_r2_prop_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61931521",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_results_df = results_summary[[\"RMSE\"]]\n",
    "linear_model_results_df[\"Model\"] = \"Multi-linear \\nRegression\"\n",
    "#linear_model_results_df = linear_model_results_df.rename(columns={\"Predictive R-squared\": \"R^2^\"})\n",
    "linear_model_results_df = linear_model_results_df[[\"Model\", \"RMSE\"]]\n",
    "\n",
    "nonlinear_model_results_df = nonlinear_model_results_df_1[[\"Model\", \"RMSE\"]]\n",
    "\n",
    "all_model_results_df = pd.concat([linear_model_results_df, nonlinear_model_results_df])\n",
    "\n",
    "\n",
    "# Calculate median R^2^ values for each model\n",
    "median_r2_group = all_model_results_df.groupby(\"Model\")[\"RMSE\"].median().sort_values()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "#plt.axhline(y=0, linewidth=3, color='black', alpha=0.1)\n",
    "\n",
    "sns.boxplot(data = all_model_results_df, x=\"Model\", y=\"RMSE\", ax=ax, color=\"lightblue\", order = median_r2_group.reset_index()[\"Model\"].to_list())\n",
    "\n",
    "plt.ylabel(\"RMSE Value\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "#plt.legend(title=\"Key\", loc=\"lower left\")\n",
    "\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.005))\n",
    "\n",
    "plt.title(\"RMSE Value of Models Predicting Proportion of Fire Damage\")\n",
    "\n",
    "plt.savefig(\"../figures/distributions/all_model_rmse_prop_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d09cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553280b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6ee8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec32d8fd",
   "metadata": {},
   "source": [
    "# Linear Model\n",
    "## Building Damage Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47051ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Create lists to store results\n",
    "building_area_coefficients = []\n",
    "building_area_r2_scores = []\n",
    "building_area_r2_score_x = []\n",
    "building_area_model_r2_scores = []\n",
    "building_area_p_values = []\n",
    "building_area_percentage_within_bounds_list = []\n",
    "real_predicted_values_ba = []\n",
    "residuals_values_ba = []\n",
    "building_area_rmse_values = [] \n",
    "\n",
    "\n",
    "# Assuming your dataset is in 'inc_df', create the feature matrix (X) and target variable (y)\n",
    "X_building_area = inc_df[['entropy', 'agency_count', 'after-action', 'casualty info', 'incident info', 'mean_temperature', 'precipitation_sum', 'windspeed', 'energyScore']]\n",
    "y_building_area = inc_df['fire_damage_area_median_value'][X_building_area.index]\n",
    "\n",
    "# Handle missing values\n",
    "X_building_area.dropna(inplace=True)\n",
    "\n",
    "# Perform one-hot encoding for the categorical column (replace 'energyScore' with the actual column name)\n",
    "X_building_area = pd.get_dummies(X_building_area, columns=['energyScore'], drop_first=True, dtype=int)\n",
    "\n",
    "# Add a constant term to the independent variables to account for the intercept in the regression\n",
    "X_building_area = sm.add_constant(X_building_area)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train_building_area, X_test_building_area, y_train_building_area, y_test_building_area = train_test_split(X_building_area, y_building_area, test_size=0.2, random_state=np.random.randint(1000))\n",
    "    \n",
    "    # Fit the multiple linear regression model\n",
    "    building_area_model = sm.OLS(y_train_building_area, X_train_building_area).fit()\n",
    "    \n",
    "    # Extract coefficients and p-values\n",
    "    building_area_model_params = building_area_model.params\n",
    "    building_area_pvals = building_area_model.pvalues\n",
    "    \n",
    "    # Get R-squared from the model\n",
    "    building_area_model_r2 = building_area_model.rsquared\n",
    "    \n",
    "    # Generate predictions for the test set\n",
    "    building_area_test_mean_predictions = building_area_model.predict(X_test_building_area)\n",
    "    \n",
    "    # Calculate R-squared based on predictive ability\n",
    "    building_area_r2_x = 1 - (np.sum((y_test_building_area - building_area_test_mean_predictions) ** 2) / np.sum((y_test_building_area - np.mean(y_test_building_area)) ** 2))\n",
    "    building_area_r2 = r2_score(y_test_building_area, building_area_test_mean_predictions)\n",
    "    \n",
    "    # Calculate the residuals for the test set\n",
    "    building_area_residuals = y_test_building_area - building_area_test_mean_predictions\n",
    "    \n",
    "    # Calculate the standard deviation of residuals (a measure of uncertainty)\n",
    "    building_area_residual_std = building_area_residuals.std()\n",
    "    \n",
    "    # Calculate RMSE for the test set\n",
    "    building_area_rmse = sqrt(mean_squared_error(y_test_building_area, building_area_test_mean_predictions))\n",
    "    \n",
    "    # Append the RMSE value to the list\n",
    "    building_area_rmse_values.append(building_area_rmse)\n",
    "    \n",
    "    # Calculate the range of predictions (within one standard deviation) for the test set\n",
    "    building_area_test_lower_bound = building_area_test_mean_predictions - building_area_residual_std\n",
    "    building_area_test_upper_bound = building_area_test_mean_predictions + building_area_residual_std\n",
    "    \n",
    "    # Count the number of predictions within the bounds\n",
    "    building_area_num_within_bounds = ((y_test_building_area >= building_area_test_lower_bound) & (y_test_building_area <= building_area_test_upper_bound)).sum()\n",
    "    \n",
    "    # Calculate the percentage of predictions within the bounds\n",
    "    building_area_percentage_within_bounds = building_area_num_within_bounds / len(y_test_building_area) * 100\n",
    "    \n",
    "    # Append results to lists\n",
    "    building_area_coefficients.append(building_area_model_params)\n",
    "    building_area_r2_scores.append(building_area_r2)\n",
    "    building_area_r2_score_x.append(building_area_r2_x)\n",
    "    building_area_model_r2_scores.append(building_area_model_r2)\n",
    "    building_area_p_values.append(building_area_pvals)\n",
    "    building_area_percentage_within_bounds_list.append(building_area_percentage_within_bounds)\n",
    "    \n",
    "    \n",
    "     # Append real and predicted values to the list\n",
    "    real_predicted_values_ba.append(pd.DataFrame({'Real': y_test_building_area, 'Predicted': building_area_test_mean_predictions}))\n",
    "    \n",
    "    # Append the residuals to the list\n",
    "    residuals_values_ba.append(pd.DataFrame({'Residuals': building_area_residuals}))\n",
    "\n",
    "# Create a DataFrame to store real and predicted values from each iteration\n",
    "real_predicted_df_ba = pd.concat(real_predicted_values_ba, axis=0)\n",
    "\n",
    "residuals_values_df_ba = pd.concat(residuals_values_ba, axis=0)\n",
    "    \n",
    "\n",
    "# Create DataFrames to store the results\n",
    "building_area_coefficients_df = pd.DataFrame(building_area_coefficients)\n",
    "building_area_p_values_df = pd.DataFrame(building_area_p_values)\n",
    "building_area_results_summary = pd.DataFrame({\n",
    "    'Predictive R-squared': building_area_r2_scores,\n",
    "    'Other Predictive R-squared': building_area_r2_score_x,\n",
    "    'Model R-squared': building_area_model_r2_scores,\n",
    "    'RMSE': building_area_rmse_values,  # Add RMSE values here\n",
    "    'Percentage within Bounds': building_area_percentage_within_bounds_list\n",
    "})\n",
    "\n",
    "# Print or analyze the DataFrames as needed\n",
    "# building_area_coefficients_df\n",
    "# building_area_p_values_df\n",
    "# building_area_results_summary\n",
    "\n",
    "# Calculate the duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Print the duration\n",
    "print(\"Total duration: {:.2f} seconds\".format(duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_predicted_df_ba = real_predicted_df_ba[(real_predicted_df_ba[\"Predicted\"] >= 0) & (real_predicted_df_ba[\"Predicted\"] <= 80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(round(real_predicted_df_ba[real_predicted_df_ba['Predicted'] > real_predicted_df_ba['Real']].shape[0] \n",
    "                / real_predicted_df_ba.shape[0] * 100, 2)) +\"% overestimated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cad4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "# Calculate the slope and intercept for the line of best fit\n",
    "slope_ba, intercept_ba = np.polyfit(real_predicted_df_ba[\"Real\"], real_predicted_df_ba['Predicted'], 1)\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(6.25))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(2.5))\n",
    "\n",
    "sns.scatterplot(y = real_predicted_df_ba[\"Predicted\"], x = real_predicted_df_ba[\"Real\"], alpha=0.05, edgecolor=\"lightblue\", color=\"None\", ax=ax)\n",
    "plt.plot([0, 200], [intercept_ba, slope_ba + intercept_ba], color='blue', linestyle='-', label= f'Predicted = {slope_ba:.2f} * Real + {intercept_ba:.2f}')\n",
    "\n",
    "plt.xlabel(\"Real\", fontsize=12)\n",
    "plt.ylabel(\"Predicted\", fontsize=12)\n",
    "\n",
    "plt.xlim(0,real_predicted_df_ba[\"Real\"].max())\n",
    "plt.ylim(0,real_predicted_df_ba[\"Predicted\"].max())\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "\n",
    "plt.title('Scatter Plot with Lines of Best Fit for Multilinear regression', y=1.05)\n",
    "plt.suptitle(\"Predicting Area of Property Damage\", y=0.92, size=10)\n",
    "\n",
    "plt.savefig(\"../figures/scatter/linear_real_predicted_area_damage.png\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c962ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Anderson-Darling test for normal distribution of residuals\n",
    "p_value_ba = normal_ad(residuals_values_df_ba[\"Residuals\"])[1]\n",
    "print(\"p-value from the test - below 0.05 generally means non-normal:\", p_value_ba)\n",
    "\n",
    "# Reporting the normality of the residuals\n",
    "if p_value_ba < 0.05:\n",
    "    print(\"Residuals are not normally distributed\")\n",
    "else:\n",
    "    print(\"Residuals are normally distributed\")\n",
    "\n",
    "# Plotting the residuals distribution\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "sns.kdeplot(residuals_values_df_ba[\"Residuals\"], color=\"lightblue\", fill=True, ax=ax)\n",
    "\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "plt.axvline(x=residuals_values_df_ba[\"Residuals\"].median(), linewidth=2, color='b', label=f\"Median: {residuals_values_df_ba['Residuals'].median():.2f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=residuals_values_df_ba[\"Residuals\"].mean(), linewidth=2, color='b', label=f\"Mean: {residuals_values_df_ba['Residuals'].mean():.2f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(12.5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.0025))\n",
    "\n",
    "plt.title(\"Distribution of risiduals for linear model predicting area of building burned\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "\n",
    "plt.savefig(\"../figures/distributions/linear_risiduals_area_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "p_value_thresh_ba=0.05\n",
    "\n",
    "print()\n",
    "if p_value > p_value_thresh_ba:\n",
    "    print('Assumption satisfied')\n",
    "else:\n",
    "    print('Assumption not satisfied')\n",
    "    print()\n",
    "    print('Confidence intervals will likely be affected')\n",
    "    print('Try performing nonlinear transformations on variables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2972bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "durbinWatson_ba = durbin_watson(residuals_values_df_ba[\"Residuals\"])\n",
    "\n",
    "# Interpretation based on the Durbin-Watson score\n",
    "if durbinWatson_ba < 1.5:\n",
    "    interpretation_ba = 'Signs of positive autocorrelation. Assumption not satisfied. Consider adding lag variables.'\n",
    "elif durbinWatson_ba > 2.5:\n",
    "    interpretation_ba = 'Signs of negative autocorrelation. Assumption not satisfied. Consider adding lag variables.'\n",
    "else:\n",
    "    interpretation_ba = 'Little to no autocorrelation. Assumption satisfied.'\n",
    "\n",
    "# Create a DataFrame\n",
    "autocorrelation_table_ba = pd.DataFrame({\n",
    "    'Durbin Watson Score': [durbinWatson_ba],\n",
    "    'Interpretation': [interpretation_ba]\n",
    "})\n",
    "\n",
    "# Round the VIF values to 2 decimal places\n",
    "autocorrelation_table_ba['Durbin Watson Score'] = autocorrelation_table_ba['Durbin Watson Score'].round(2)\n",
    "\n",
    "# Print the DataFrame\n",
    "autocorrelation_table_ba\n",
    "\n",
    "pandas_to_markdown(autocorrelation_table_ba, \"linear_durbinWatson_area_damage\", \"The table displays the calculated Durbin-Watson score and its corresponding interpretation based on autocorrelation assumptions. The Durbin-Watson score measures the presence of autocorrelation in the model residuals. Note: The values presented are rounded to two decimal places.\", \"tbl-linear-durbinWatson-area-damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.scatterplot(data = residuals_values_df_ba, x = residuals_values_df_ba.index, y=\"Residuals\", alpha=0.05, edgecolor=\"lightblue\", color=\"None\", ax = ax)\n",
    "plt.axhline(y = 0, color='darkorange', linestyle='--')\n",
    "\n",
    "plt.ylim(-175, 175)\n",
    "\n",
    "plt.xlabel(\"Data Point Index\", fontsize=12)\n",
    "plt.ylabel(\"Residuals\", fontsize=12)\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(12.5))\n",
    "#ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "\n",
    "plt.title(\"Distribution of Residuals for Linear Prediction of Area of Building Damage\")\n",
    "\n",
    "plt.savefig(\"../figures/scatter/linear_risiduals_area_damage.png\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd1a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60daa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdcae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f74a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bffbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.2)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "sns.kdeplot(data = building_area_results_summary, x = \"Predictive R-squared\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Predictive $R^2$ Value\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "\n",
    "median_r2 = building_area_results_summary[\"Predictive R-squared\"].median()\n",
    "mean_r2 = building_area_results_summary[\"Predictive R-squared\"].mean()\n",
    "\n",
    "plt.axvline(x=median_r2, linewidth=2, color='b', label=f\"Median: {median_r2:.3f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_r2, linewidth=2, color='b', label=f\"Mean: {mean_r2:.3f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.0625))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/linear_r2_damage_area.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_area_rounded_coefficients_df = building_area_coefficients_df.describe().T.round(3)\n",
    "building_area_rounded_coefficients_df.columns = building_area_rounded_coefficients_df.columns.str.capitalize()\n",
    "building_area_rounded_coefficients_df.drop(columns=['Count'], inplace=True)\n",
    "building_area_rounded_coefficients_df = building_area_rounded_coefficients_df.reset_index()\n",
    "building_area_rounded_coefficients_df = building_area_rounded_coefficients_df[[\"index\", \"Mean\", \"50%\",\"Std\"]]\n",
    "building_area_rounded_coefficients_df = building_area_rounded_coefficients_df.rename(columns={\"index\": \"Independent Variable\", \n",
    "                                                                  \"Mean\":\"Mean Coefficient Value\",\n",
    "                                                                 \"50%\": \"Median Coefficient Value\",\n",
    "                                                                 \"Std\": \"Standard Deviation\"})\n",
    "\n",
    "\n",
    "\n",
    "pandas_to_markdown(building_area_rounded_coefficients_df, \"linear_coefficients_damage_area\", \"Table presenting summary statistics of coefficients obtained from a multiple linear regression analysis. The statistics presented offer insight into the distribution and central tendency of the coefficients, allowing the relationship of independent and target variables to be discovered. Note: The values presented are rounded to three decimal places.\", \"tbl-linear_coefficients_damage_area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd59e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ac97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bef7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d08d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e21b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43337a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ebd80df",
   "metadata": {},
   "source": [
    "## Non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d8944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a timer\n",
    "start_time_building_area = time.time()\n",
    "\n",
    "# Create lists to store results\n",
    "model_names_building_area = []\n",
    "mse_values_building_area = []\n",
    "rmse_values_building_area = []\n",
    "r2_values_building_area = []\n",
    "\n",
    "\n",
    "\n",
    "# Specify the models and their corresponding constructors\n",
    "models = [\n",
    "    #('Neural Network', MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)),\n",
    "    ('Random Forest', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "    ('SVR', SVR(kernel='rbf', C=1.0, epsilon=0.1)),\n",
    "    ('KNN Regression', KNeighborsRegressor(n_neighbors=5)),\n",
    "    ('XGBoost', XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3))\n",
    "]\n",
    "\n",
    "# Iterate through the models\n",
    "for model_name, model in models:\n",
    "    # Create a list to store model results for each iteration\n",
    "    model_results_building_area = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Split the data into training and testing sets (80% train, 20% test)\n",
    "        X_train_nonlinear_building_area, X_test_nonlinear_building_area, y_train_nonlinear_building_area, y_test_nonlinear_building_area = train_test_split(X_building_area, y_building_area, test_size=0.2)\n",
    "\n",
    "        # Fit the model and make predictions\n",
    "        model.fit(X_train_nonlinear_building_area, y_train_nonlinear_building_area)\n",
    "        y_pred_nonlinear_building_area = model.predict(X_test_nonlinear_building_area)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse_nonlinear_building_area = mean_squared_error(y_test_nonlinear_building_area, y_pred_nonlinear_building_area)\n",
    "        rmse_nonlinear_building_area = mean_squared_error(y_test_nonlinear_building_area, y_pred_nonlinear_building_area, squared=False)\n",
    "        r2_nonlinear_building_area = r2_score(y_test_nonlinear_building_area, y_pred_nonlinear_building_area)\n",
    "\n",
    "        # Append results to the model_results list\n",
    "        model_results_building_area.append((mse_nonlinear_building_area, rmse_nonlinear_building_area, r2_nonlinear_building_area))\n",
    "\n",
    "    # Append results to the respective lists\n",
    "    model_names_building_area.extend([model_name] * num_iterations)\n",
    "    mse_values_building_area.extend([result[0] for result in model_results_building_area])\n",
    "    rmse_values_building_area.extend([result[1] for result in model_results_building_area])\n",
    "    r2_values_building_area.extend([result[2] for result in model_results_building_area])\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "nonlinear_model_results_df_building_area = pd.DataFrame({\n",
    "    'Model': model_names_building_area,\n",
    "    'R^2^': r2_values_building_area,\n",
    "    'MSE': mse_values_building_area,\n",
    "    'RMSE': rmse_values_building_area\n",
    "})\n",
    "\n",
    "# Round the values\n",
    "nonlinear_model_results_df_building_area = nonlinear_model_results_df_building_area.round(3)\n",
    "\n",
    "# Display the DataFrame\n",
    "nonlinear_model_results_df_building_area\n",
    "\n",
    "# Calculate the duration\n",
    "end_time_building_area = time.time()\n",
    "duration_building_area = end_time_building_area - start_time_building_area\n",
    "\n",
    "# Print the duration\n",
    "print(\"Total duration: {:.2f} seconds\".format(duration_building_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c73a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model_results_df_building_area_1 = nonlinear_model_results_df_building_area.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_results_df_building_area = building_area_results_summary[[\"Predictive R-squared\"]]\n",
    "linear_model_results_df_building_area[\"Model\"] = \"Multi-linear \\nRegression\"\n",
    "linear_model_results_df_building_area = linear_model_results_df_building_area.rename(columns={\"Predictive R-squared\": \"R^2^\"})\n",
    "linear_model_results_df_building_area = linear_model_results_df_building_area[[\"Model\", \"R^2^\"]]\n",
    "\n",
    "nonlinear_model_results_df_building_area = nonlinear_model_results_df_building_area[[\"Model\", \"R^2^\"]]\n",
    "\n",
    "all_model_results_df_building_area = pd.concat([linear_model_results_df_building_area, nonlinear_model_results_df_building_area])\n",
    "\n",
    "\n",
    "# Calculate median R^2^ values for each model\n",
    "median_r2_group_building_area = all_model_results_df_building_area.groupby(\"Model\")[\"R^2^\"].median().sort_values()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "#plt.axhline(y=0, linewidth=3, color='black', alpha=0.1)\n",
    "\n",
    "sns.boxplot(data = all_model_results_df_building_area, x=\"Model\", y=\"R^2^\", ax=ax, color=\"lightblue\", order = median_r2_group_building_area.reset_index()[\"Model\"].to_list())\n",
    "\n",
    "plt.ylabel(\"Predictive $R^2$ Value\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "#plt.legend(title=\"Key\", loc=\"lower left\")\n",
    "\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.125))\n",
    "\n",
    "plt.title(\"$R^2$ Value of Models Predicting Area of Fire Damage\")\n",
    "\n",
    "plt.ylim(-2, 0.6)\n",
    "\n",
    "plt.savefig(\"../figures/distributions/all_model_r2_area_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_results_df_building_area_rmse = building_area_results_summary[[\"RMSE\"]]\n",
    "linear_model_results_df_building_area_rmse[\"Model\"] = \"Multi-linear \\nRegression\"\n",
    "#linear_model_results_df_building_area = linear_model_results_df_building_area.rename(columns={\"Predictive R-squared\": \"R^2^\"})\n",
    "linear_model_results_df_building_area_rmse = linear_model_results_df_building_area_rmse[[\"Model\", \"RMSE\"]]\n",
    "\n",
    "nonlinear_model_results_df_building_area_rmse = nonlinear_model_results_df_building_area_1[[\"Model\", \"RMSE\"]]\n",
    "\n",
    "all_model_results_df_building_area_rmse = pd.concat([linear_model_results_df_building_area_rmse, nonlinear_model_results_df_building_area_rmse])\n",
    "\n",
    "\n",
    "# Calculate median RMSE values for each model\n",
    "median_r2_group_building_area_rmse = all_model_results_df_building_area_rmse.groupby(\"Model\")[\"RMSE\"].median().sort_values()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.4)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "#plt.axhline(y=0, linewidth=3, color='black', alpha=0.1)\n",
    "\n",
    "sns.boxplot(data = all_model_results_df_building_area_rmse, x=\"Model\", y=\"RMSE\", ax=ax, color=\"lightblue\", order = median_r2_group_building_area.reset_index()[\"Model\"].to_list())\n",
    "\n",
    "plt.ylabel(\"RMSE Value\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "plt.title(\"RMSE Value of Models Predicting Area of Fire Damage\")\n",
    "\n",
    "plt.savefig(\"../figures/distributions/all_model_rmse_area_damage.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0f628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658c6c21",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc66fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_results_df[\"Estimating\"] = \"Proportion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_results_df_building_area[\"Estimating\"] = \"Area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9f4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_model_results_df_building_area, all_model_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54cb1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear = all_results[all_results[\"Model\"] == \"Multi-linear \\nRegression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8675e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "\n",
    "plt.grid(which='major', color='black', linewidth=0.5, alpha=0.2)\n",
    "plt.grid(which='minor', color='black', linewidth=0.25, alpha=0.2)\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.125))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.125))\n",
    "\n",
    "\n",
    "sns.kdeplot(data = linear, x=\"R^2^\", hue=\"Estimating\", ax = ax, fill = False)\n",
    "\n",
    "plt.axvline(x=1, linewidth=2, color='b', label=\"Area of Damage\", alpha=1, linestyle=\"-\")\n",
    "plt.axvline(x=1, linewidth=2, color='orange', label=\"Proportion of Damage\", alpha=1, linestyle=\"-\")\n",
    "\n",
    "\n",
    "plt.xlim(-1.5, 0.75)\n",
    "\n",
    "median_r2_area = linear[linear[\"Estimating\"] == \"Area\"][\"R^2^\"].median()\n",
    "mean_r2_area = linear[linear[\"Estimating\"] == \"Area\"][\"R^2^\"].mean()\n",
    "\n",
    "plt.axvline(x=median_r2, linewidth=2, color='b', label=f\"Median: {median_r2_area:.3f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_r2, linewidth=2, color='b', label=f\"Mean: {mean_r2_area:.3f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "median_r2_prop = linear[linear[\"Estimating\"] == \"Proportion\"][\"R^2^\"].median()\n",
    "mean_r2_prop = linear[linear[\"Estimating\"] == \"Proportion\"][\"R^2^\"].mean()\n",
    "\n",
    "plt.axvline(x=median_r2, linewidth=2, color='orange', label=f\"Median: {median_r2_prop:.3f}\", alpha=0.5, linestyle=\":\")\n",
    "plt.axvline(x=mean_r2, linewidth=2, color='orange', label=f\"Mean: {mean_r2_prop:.3f}\", alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "plt.title(\"KDE Plot Comparing the Performance of Both Models\")\n",
    "\n",
    "plt.legend(title=\"Key\")\n",
    "plt.savefig(\"../figures/distributions/all_r2.pdf\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration\n",
    "end_time_all = time.time()\n",
    "duration_all = end_time_all - start_time_all\n",
    "\n",
    "# Print the duration\n",
    "print(\"Total duration: {:.2f} seconds\".format(duration_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
